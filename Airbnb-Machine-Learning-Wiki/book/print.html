<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Airbnb-Machine-Learning-Wiki</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Explanation</li><li class="spacer"></li><li class="chapter-item expanded "><a href="src/1-Introduction/about-me.html"><strong aria-hidden="true">1.</strong> About me</a></li><li class="chapter-item expanded "><a href="src/1-Introduction/index.html"><strong aria-hidden="true">2.</strong> Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="src/1-Introduction/objective.html"><strong aria-hidden="true">2.1.</strong> Context and objective</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><li class="part-title">Getting Started</li><li class="chapter-item expanded "><a href="src/2-Importing-and-installing-libraries/index.html"><strong aria-hidden="true">3.</strong> Importing and installing libraries</a></li><li class="chapter-item expanded "><a href="src/3-Importing-and-consolidating-database/importing-and-consolidating-database.html"><strong aria-hidden="true">4.</strong> Importing and the consolidating databse</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Treating database</li><li class="chapter-item expanded "><a href="src/4-Reduce-excessive-columns/index.html"><strong aria-hidden="true">5.</strong> Steps underwent in treating the data</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="src/4-Reduce-excessive-columns/reduce-excessive-collumns.html"><strong aria-hidden="true">5.1.</strong> Reducing excessive amount of columns</a></li><li class="chapter-item expanded "><a href="src/4-Reduce-excessive-columns/treating-missing-values.html"><strong aria-hidden="true">5.2.</strong> Treating missing values</a></li><li class="chapter-item expanded "><a href="src/4-Reduce-excessive-columns/verify-data-types.html"><strong aria-hidden="true">5.3.</strong> Verifying data types of each collumn</a></li></ol></li><li class="chapter-item expanded "><a href="src/5-Treatment-of-outliers/index.html"><strong aria-hidden="true">6.</strong> Treatment of outliers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="src/5-Treatment-of-outliers/analyzing-heatmap.html"><strong aria-hidden="true">6.1.</strong> Analyzing heatmap</a></li><li class="chapter-item expanded "><a href="src/5-Treatment-of-outliers/calculating-limits.html"><strong aria-hidden="true">6.2.</strong> Calculating limits</a></li><li class="chapter-item expanded "><a href="src/5-Treatment-of-outliers/creating-graphs.html"><strong aria-hidden="true">6.3.</strong> Creating graphs</a></li><li class="chapter-item expanded "><a href="src/5-Treatment-of-outliers/5-1-Dropping-uncessary-columns/index.html"><strong aria-hidden="true">6.4.</strong> Dropping unecessary columns</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="src/5-Treatment-of-outliers/5-1-Dropping-uncessary-columns/guests_included.html"><strong aria-hidden="true">6.4.1.</strong> 'guests_included'</a></li><li class="chapter-item expanded "><a href="src/5-Treatment-of-outliers/5-1-Dropping-uncessary-columns/number_of_reviews.html"><strong aria-hidden="true">6.4.2.</strong> 'number_of_reviews'</a></li><li class="chapter-item expanded "><a href="src/5-Treatment-of-outliers/5-1-Dropping-uncessary-columns/maximum_nights.html"><strong aria-hidden="true">6.4.3.</strong> 'maximum_nights'</a></li></ol></li><li class="chapter-item expanded "><a href="src/6-Outlier-function/index.html"><strong aria-hidden="true">6.5.</strong> Outlier function</a></li><li class="chapter-item expanded "><a href="src/7-Removing-outliers-price-extra-people/index.html"><strong aria-hidden="true">6.6.</strong> Removing outliers from the 'price' and 'extra_people' columns</a></li><li class="chapter-item expanded "><a href="src/8-outliers-discrete-numerical-columns/index.html"><strong aria-hidden="true">6.7.</strong> Removing outliers of discrete numerical columns</a></li></ol></li><li class="chapter-item expanded "><a href="src/9-Treating-non-numerical-values/index.html"><strong aria-hidden="true">7.</strong> Treating non numerical values</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="src/10-Group_categories-function/index.html"><strong aria-hidden="true">7.1.</strong> Group_categories function</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="src/10-Group_categories-function/10-1-group-property_type/index.html"><strong aria-hidden="true">7.1.1.</strong> group property_type</a></li><li class="chapter-item expanded "><a href="src/10-Group_categories-function/10-2-group-bed_type/index.html"><strong aria-hidden="true">7.1.2.</strong> group bed_type</a></li><li class="chapter-item expanded "><a href="src/10-Group_categories-function/10-3-group-cancellation-policy/index.html"><strong aria-hidden="true">7.1.3.</strong> group cancellation_policy</a></li><li class="chapter-item expanded "><a href="src/10-Group_categories-function/10-4-group-room_type/index.html"><strong aria-hidden="true">7.1.4.</strong> group room_type</a></li></ol></li><li class="chapter-item expanded "><a href="src/11-Treating-ammenities-collumn/index.html"><strong aria-hidden="true">7.2.</strong> Treating 'ammenities' collumn</a></li><li class="spacer"></li></ol></li><li class="chapter-item expanded "><li class="part-title">Visualizing map</li><li class="chapter-item expanded "><a href="src/12-reducing-visualized-data/index.html"><strong aria-hidden="true">8.</strong> Reducing visualized data</a></li><li class="chapter-item expanded "><a href="src/13-loading-map-to-browser/index.html"><strong aria-hidden="true">9.</strong> Loading the map into the browser</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Encoding</li><li class="chapter-item expanded "><a href="src/14-Encoding-explanation/index.html"><strong aria-hidden="true">10.</strong> Encoding explanation</a></li><li class="chapter-item expanded "><a href="src/15-Encoding-booleans/index.html"><strong aria-hidden="true">11.</strong> Encoding booleans</a></li><li class="chapter-item expanded "><a href="src/16-dummy-encoding/index.html"><strong aria-hidden="true">12.</strong> Encoding text columns (dummy encoding)</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Prediction models</li><li class="chapter-item expanded "><a href="src/17-steps-to-build-model/index.html"><strong aria-hidden="true">13.</strong> 7 steps to build a prediction model</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="src/18-define-classification-regression-problem/index.html"><strong aria-hidden="true">13.1.</strong> Defining if it is classification or regression problem</a></li><li class="chapter-item expanded "><a href="src/19-Choosing-evaluation-metrics/index.html"><strong aria-hidden="true">13.2.</strong> Choosing the metrics to evaluate the model</a></li><li class="chapter-item expanded "><a href="src/20-choose-models-to-use/index.html"><strong aria-hidden="true">13.3.</strong> Choose which models we are going to use</a></li><li class="chapter-item expanded "><a href="src/21-Training-and-testing-models/index.html"><strong aria-hidden="true">13.4.</strong> Train the models and test</a></li><li class="chapter-item expanded "><a href="src/22-Compare-model-results/index.html"><strong aria-hidden="true">13.5.</strong> Comparing the results of the models and choosing the best one</a></li><li class="chapter-item expanded "><a href="src/23-analyze-best-model/index.html"><strong aria-hidden="true">13.6.</strong> Analyzing the best model</a></li><li class="chapter-item expanded "><a href="src/24-Adjust-and-improve-model/index.html"><strong aria-hidden="true">13.7.</strong> Adjusting and improving the best model</a></li></ol></li><li class="chapter-item expanded "><a href="src/25-analyze-model-function/index.html"><strong aria-hidden="true">14.</strong> analyze_model function</a></li><li class="chapter-item expanded "><a href="src/26-splitting-the-data/index.html"><strong aria-hidden="true">15.</strong> Splitting the data</a></li><li class="chapter-item expanded "><a href="src/27-testing-the-models/index.html"><strong aria-hidden="true">16.</strong> Testing the three models</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Final model</li><li class="chapter-item expanded "><a href="src/28-choosing-best-model-final/index.html"><strong aria-hidden="true">17.</strong> Choosing the best model</a></li><li class="chapter-item expanded "><a href="src/29-analyzing-best-model-final/index.html"><strong aria-hidden="true">18.</strong> Analyzing the best model</a></li><li class="chapter-item expanded "><a href="src/30-adjusting-and-imporivng-final/index.html"><strong aria-hidden="true">19.</strong> Adjusting an improving the best model</a></li><li class="chapter-item expanded "><a href="src/31-applying-final-changes/index.html"><strong aria-hidden="true">20.</strong> Applying the final changes</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Deployment (streamlit)</li><li class="chapter-item expanded "><a href="src/32-deployment-forms/index.html"><strong aria-hidden="true">21.</strong> Deployment forms</a></li><li class="chapter-item expanded "><a href="src/33-exporting-model-joblib/index.html"><strong aria-hidden="true">22.</strong> Exporting model as joblib</a></li><li class="chapter-item expanded "><a href="src/34-airbnb-deploy-file/index.html"><strong aria-hidden="true">23.</strong> airbnb_deploy file</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="src/35-importing-and-creating-button/index.html"><strong aria-hidden="true">23.1.</strong> Importing streamlit and setting dictionaries</a></li><li class="chapter-item expanded "><a href="src/36-dummy-variable-dictionaries/index.html"><strong aria-hidden="true">23.2.</strong> Setting up page and config</a></li></ol></li><li class="chapter-item expanded "><a href="src/37-attributing-values-inputs/index.html"><strong aria-hidden="true">24.</strong> Creating the buttons</a></li><li class="chapter-item expanded "><a href="src/38-preview-value-button/index.html"><strong aria-hidden="true">25.</strong> Creating the preview value button</a></li><li class="chapter-item expanded "><a href="src/39-finalizing/index.html"><strong aria-hidden="true">26.</strong> Finalizing</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Conclusion</li><li class="chapter-item expanded "><a href="src/40-thanks-future-plans/index.html"><strong aria-hidden="true">27.</strong> Thanks and future plans</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Airbnb-Machine-Learning-Wiki</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="about-me"><a class="header" href="#about-me">About me</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Wikipedia explanation</p>
<p>Social media</p>
<p>Focus of project on machine learning</p>
<p>Types of machine learning (which one we chose)</p>
<p>Mention cautions such as overfitting to limit the data</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="context-and-objective"><a class="header" href="#context-and-objective">Context and Objective</a></h1>
<p>Airbnb allows anyone with a spare room or property of any type (apartment, house, chalet, inn, etc.) to list their property for rent on a daily basis.</p>
<p>As a host, you create your profile and list your property. In this listing, hosts should provide a comprehensive description of the property to assist renters/travelers in choosing the best accommodation and to make their listing more appealing.</p>
<p>There are numerous customizations available in the listing, ranging from minimum stay requirements, pricing, number of rooms, to cancellation policies, extra guest fees, identity verification requirements for renters, etc.</p>
<h3 id="our-objective"><a class="header" href="#our-objective">Our Objective</a></h3>
<p>To build a price prediction model that enables property owners to determine the appropriate daily rate for their property. Additionally, to assist renters in evaluating whether a listed property offers a competitive price compared to similar properties with similar characteristics.</p>
<h3 id="available-resources-inspirations-and-credits"><a class="header" href="#available-resources-inspirations-and-credits">Available Resources, Inspirations, and Credits</a></h3>
<p>The datasets were sourced from Kaggle: <a href="https://www.kaggle.com/datasets/allanbruno/airbnb-rio-de-janeiro">https://www.kaggle.com/datasets/allanbruno/airbnb-rio-de-janeiro</a>. Data spans from April 2018 to May 2020, with the exception of June 2018, which lacks data.</p>
<p>Given the 50MB per file space restriction in GitHub Repositories, the datasets utilized in this project are accessible for download via this link: <a href="https://drive.google.com/file/d/1_RtxDTXtF3CGvioFi1_ophzLNmyguEHl/view?usp=sharing">https://drive.google.com/file/d/1_RtxDTXtF3CGvioFi1_ophzLNmyguEHl/view?usp=sharing</a>. Alternatively, you may procure the datasets directly from Kaggle. However, it's noteworthy that discrepancies may arise in results if the datasets have been updated subsequent to the project's inception.</p>
<ul>
<li>File names are in brazilian portuguese</li>
<li>The datasets contain property prices and their respective characteristics for each month.</li>
<li>Prices are listed in Brazilian Real (R$).</li>
</ul>
<h3 id="initial-expectations"><a class="header" href="#initial-expectations">Initial Expectations</a></h3>
<ul>
<li>Seasonality is expected to be a significant factor, as months like December tend to have higher prices in Rio de Janeiro.</li>
<li>Property location is likely to have a substantial impact on pricing, given that location can drastically alter the characteristics of a place (e.g., safety, natural beauty, proximity to tourist attractions).</li>
<li>Additional amenities may have a significant impact, considering the prevalence of older buildings and houses in Rio de Janeiro.</li>
</ul>
<p>We aim to explore the extent to which these factors influence pricing and identify any less intuitive yet crucial factors.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing-libraries"><a class="header" href="#installing-libraries">Installing libraries</a></h1>
<p>Download the requirements.txt and in the command prompt run:</p>
<pre><code>pip install requirements.txt
</code></pre>
<h1 id="importing-libraries"><a class="header" href="#importing-libraries">Importing libraries</a></h1>
<pre><code class="language-python">#? for path management
import os
import pathlib
import joblib
#? for data manipulation
import pandas as pd 
import numpy as np
#? for data visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import webbrowser
#? for machine learning
from sklearn.metrics import r2_score, root_mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.model_selection import train_test_split

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="importing-and-consolidating-the-database"><a class="header" href="#importing-and-consolidating-the-database">Importing and consolidating the database</a></h1>
<p>The dataset comprises 25 databases stored in separate tables. To facilitate data processing and facilitate machine analysis, a consolidated database termed "main_dataframe" was created by integrating all these databases. This integration process involved the utilization of the Python pandas library, wherein each database, stored as a CSV file, was imported individually and appended to an initially empty list named "df_list". Subsequently, the pandas function pd.concat() was employed to concatenate all individual dataframes into a singular cohesive dataset.</p>
<p>Furthermore, each dataframe within the main_dataframe contains a column representing dates. However, the dates are inconsistently formatted strings. To address this inconsistency, an extraction process was implemented to isolate the month and year components from the dataframe names. These extracted components were then utilized to create corresponding columns within the main_dataframe, thus ensuring consistency and facilitating further analytical operations.</p>
<pre><code class="language-python">dataset_folder = pathlib.Path('dataset')
months = {'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4, 'mai': 5, 'jun': 6, 'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12}
df_list = []


#* concatenating all the databases into one while adding the month and year columns
for file in dataset_folder.iterdir() :
    month = months[file.name[:3]]

    year = int(file.name[-8:].replace('.csv', ''))

    df = pd.read_csv(file)
    df['month'] = month  
    df['year'] = year

    df_list.append(df)

main_dataframe = pd.concat(df_list)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>mencionar o slide dos passos</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reducing-excessive-amount-of-columns"><a class="header" href="#reducing-excessive-amount-of-columns">Reducing excessive amount of columns</a></h1>
<p>As there are a lot of columns, we are going to reduce the amount of columns to enhance the algorithm efficiency.</p>
<p>Furthermore, a rapid analysis of the data shows a significant number of redundant columns in the data for the prediction model, leading to the removal of the following columns:</p>
<ol>
<li>Any collumn with unecessary data types that wont influence the final price such as images, verification methods, etc.</li>
<li>ID: these quantitative values are unnecessary and could interfere with the final results as they would also be calculated.</li>
<li>Repeated columns: the data contains many columns that are repeated or very similar to each other, such as the date, state, and country.</li>
<li>Any collumn that contains hyperlinks or free-form text: besides of not containing relevant data for the desired result, it could interfere with the prediction model.</li>
<li>If over 30% of the data is missing we will remove that collumn</li>
</ol>
<p>After this analisis we were left with the following collumns:</p>
<pre><code class="language-python">filtered_columns = ['host_response_time','host_response_rate','host_is_superhost','host_listings_count','latitude','longitude','property_type','room_type','accommodates','bathrooms','bedrooms','beds','bed_type','amenities','price','security_deposit','cleaning_fee','guests_included','extra_people','minimum_nights','maximum_nights','number_of_reviews','review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value','instant_bookable','is_business_travel_ready','cancellation_policy','year','month']

main_dataframe = main_dataframe.loc[:, filtered_columns]
</code></pre>
<p>A excel file with the first 900 rows was generated in order to do a quick analysis of the data.</p>
<pre><code class="language-python">main_dataframe.head(900).to_csv('first_900_rows.csv', sep=';', index=False)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="treating-missing-values"><a class="header" href="#treating-missing-values">Treating missing values</a></h1>
<p>Utilizing the command:</p>
<pre><code class="language-python">print(base_airbnb.isnull().sum())
</code></pre>
<p>We can see the amount of null values in each column:</p>
<p><img src="src%5C4-Reduce-excessive-columns/image/treating-missing-values/1712176606720.png" alt="1712176606720" /></p>
<p>If over 30% of the data is missing we will remove that collumn to optimize the model . Upon visualizing the dataset, it became apparent that certain columns contain a substantial amount of null values. Columns with over 30% of missing data will be entirely discarded, while those with fewer null values will undergo null value removal to ensure data integrity.</p>
<pre><code class="language-python">row_30_percent = main_dataframe.shape[0] * 0.3
for collumn in main_dataframe :
    if main_dataframe[collumn].isnull().sum() &gt;= row_30_percent :
        main_dataframe = main_dataframe.drop(collumn, axis=1)
</code></pre>
<p>After removing said columns we were left with these:</p>
<pre><code class="language-python">print(base_airbnb.isnull().sum())
</code></pre>
<p><img src="src%5C4-Reduce-excessive-columns/image/treating-missing-values/1712179151121.png" alt="1712179151121" /></p>
<p>As there aren't a significant amount of null values left, we will remove them.</p>
<pre><code class="language-python">main_dataframe = main_dataframe.dropna()
</code></pre>
<p>In the end, all columns left won't have any null values, effectively optimizing our dataset for further analysis, thereby maximizing the accuracy and reliability of our results.</p>
<p><img src="src%5C4-Reduce-excessive-columns/image/treating-missing-values/1712179751324.png" alt="1712179751324" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="verifying-the-data-types-of-each-collumn"><a class="header" href="#verifying-the-data-types-of-each-collumn">Verifying the data types of each collumn</a></h1>
<p>Following inspection the data types of each column, it is evident that the majority conform to their intended data types. However, the 'price' and 'extra people' columns are incorrectly represented as objects (strings) rather than integers as expected, necessitating a conversion to their appropriate data type.</p>
<p>In order to observe the data types we utilized the following command:</p>
<pre><code class="language-python">print(base_dataframe.dtypes)
print('-'*60)
print(base_dataframe.iloc[0])
</code></pre>
<p><img src="src%5C4-Reduce-excessive-columns/image/verify-data-types/1712180387610.png" alt="1712180387610" /></p>
<p>Additionally, all data types of float64 and int64 will be converted to their 32-bit variants to optimize memory usage.</p>
<pre><code class="language-python">for collumn in main_dataframe :
    if main_dataframe[collumn].dtype == 'float64' :
        main_dataframe[collumn] = main_dataframe[collumn].astype(np.float32)
    elif main_dataframe[collumn].dtype == 'int64' :
        main_dataframe[collumn] = main_dataframe[collumn].astype(np.int32)

for collumn in ['price', 'extra_people'] :
    main_dataframe[collumn] = main_dataframe[collumn].str.replace(',', '').str.replace('$', '').astype(np.float32, copy=False) #used np.float32 to reduce memory usage
</code></pre>
<p>After the conversions the 'price' and 'extra_people' columns were effectively converted to float32 alongside with all the other columns that were int64 or int32, who where converted to their 32 bit variants</p>
<p><img src="src%5C4-Reduce-excessive-columns/image/verify-data-types/1712181195726.png" alt="1712181195726" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="exploratory-analysis-and-treatment-of-outliers-for-numerical-values"><a class="header" href="#exploratory-analysis-and-treatment-of-outliers-for-numerical-values">Exploratory analysis and treatment of outliers for numerical values</a></h1>
<ul>
<li>
<p>We will essentially examine each feature to:</p>
<ol>
<li>Conduct a correlation analysis among the features to ascertain their interrelationships and determine whether to retain all features. Features exhibiting strong correlations to the extent that they provide redundant information to the model will be removed.</li>
<li>Eliminate outliers (using a rule where values below Q1 - 1.5 * Interquartile Range and above Q3 + 1.5  Interquartile Range will be excluded). Interquartile Range (IQR) = Q3 - Q1.</li>
<li>Verify if all features are relevant for our model or if any of them will not contribute and should be removed.</li>
</ol>
</li>
<li>
<p>We will begin by creating graphs to facilitate parts of our analysis</p>
</li>
<li>
<p>Then, we will analyize the columns of price (the ultimate target variable) and extra_people (also a monetary value). These are continuous numerical values.</p>
</li>
<li>
<p>Next, we will analyze the columns of discrete numerical values (accommodates, bedrooms, guests_included, etc.).</p>
</li>
<li>
<p>Finally, we will evaluate the text columns and determine which categories make sense to retain or discard.</p>
</li>
</ul>
<p><strong>NOTE:</strong> If x axis of the graphs is slightly cropped off the screen due to the size of the graph, go to the configure subplots button (second to last  button located on the bottom left of the screen) and adjust the 'bottom' slider until the x axis appears on your desired position.
<img src="src%5C5-Treatment-of-outliers/image/README/1713012845818.png" alt="1713012845818" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analyzing-heatmap"><a class="header" href="#analyzing-heatmap">Analyzing heatmap</a></h1>
<p>In order to perform a correlation analysis among the features we will create a heatmap of the correlation coefficient of each feature:</p>
<pre><code class="language-python">#* Making a heatmap from the correlation coefficient
plt.figure(figsize=(15,10))
plt.subplots_adjust(bottom=0.264)
sns.heatmap(main_dataframe.corr(numeric_only=True), annot=True)
plt.show()
</code></pre>
<p><img src="src%5C5-Treatment-of-outliers/image/analyzing-heatmap/1712425227663.png" alt="1712425227663" /></p>
<p>None of the correlation coefficients observed among the features reached a strength indicative of redundancy for the prediction model (excluding the coefficient of 1 present in the comparison of same features).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="calculating-limits"><a class="header" href="#calculating-limits">Calculating limits</a></h1>
<p>We will create a function called 'calculate_limits' that will recieve a collumn from the main_dataframe as a parameter and it will return to us the outliers below the first quartile and those above the third quartile as mentioned in the <a href="src%5C5-Treatment-of-outliers/README.html">second step of how to locate outliers</a></p>
<pre><code class="language-python">def calculate_limits(collumn) :
    q1 = collumn.quantile(0.25)
    q3 = collumn.quantile(0.75)
    iqr = q3 - q1
    return q1 - 1.5 * iqr, q3 + 1.5 * iqr

# print(calculate_limits(main_dataframe['price']))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="creating-graphs"><a class="header" href="#creating-graphs">Creating graphs</a></h1>
<p>Now we will create four functions, each for visualizing a different type of graph and test them with the 'guests_included' column (except with the bar graph for string data):</p>
<ol>
<li>
<p>Bar Graph for string data</p>
<pre><code class="language-python">def bar_graph_string(main_dataframe_collumn_name) :
    plt.figure(figsize=(15, 5))
    plt.subplots_adjust(bottom=0.3)
    bar_graph = sns.countplot(x=main_dataframe_collumn_name, data=main_dataframe)
    bar_graph.tick_params(axis='x', rotation=90)
    plt.show()

bar_graph_string('bed_type')
</code></pre>
<p><img src="src%5C5-Treatment-of-outliers/image/creating-graphs/1713011717325.png" alt="1713011717325" /></p>
<p>Now, the graphs below will only work properly for NUMERICAL data, as they utilize limits determined by quartiles. Also, notice how it will be necessary to pass the whole column as a parameter (ex: main_dataframe['guests_included']) instead of only passing the string name of the column as done in the bar graph for strings that recieved just the name string 'bed_type'.</p>
</li>
<li>
<p>Box plot</p>
</li>
</ol>
<pre><code class="language-python">   def box_plot(main_dataframe_collumn) :
       plt.figure(figsize=(15,5))
       plt.subplots_adjust(bottom=0.3)
       sns.boxplot(x = main_dataframe_collumn)
       plt.show()


   box_plot(main_dataframe['guests_included'])
</code></pre>
<p><img src="src%5C5-Treatment-of-outliers/image/creating-graphs/1712437504057.png" alt="1712437504057" /></p>
<ol start="3">
<li>Histogram</li>
</ol>
<pre><code class="language-python">   def histogram(main_dataframe_collumn) :
       plt.figure(figsize=(15, 5))
       plt.subplots_adjust(bottom=0.3)
       sns.histplot(x = main_dataframe_collumn, kde=True)
       plt.show()

   histogram(main_dataframe['guests_included'])
</code></pre>
<p><img src="src%5C5-Treatment-of-outliers/image/creating-graphs/1712437556071.png" alt="1712437556071" /></p>
<ol start="4">
<li>Bar graph</li>
</ol>
<pre><code class="language-python">   def bar_graph(main_dataframe_collumn) :
       plt.figure(figsize=(15, 5))
       plt.subplots_adjust(bottom=0.3)
       ax = sns.barplot(x = main_dataframe_collumn.value_counts().index, y = main_dataframe_collumn.value_counts())
       ax.set_xlim(calculate_limits(main_dataframe_collumn))
       plt.show()

   bar_graph(main_dataframe['guests_included'])
</code></pre>
<p><img src="src%5C5-Treatment-of-outliers/image/creating-graphs/1712437589513.png" alt="1712437589513" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="removing-unecessary-columns"><a class="header" href="#removing-unecessary-columns">Removing unecessary columns</a></h1>
<p>After analyzing the graphs we decided to remove the following columns from the main_database:</p>
<ol>
<li>'guests_included'</li>
<li>'number_of_reviews'</li>
<li>'maximum_nights'</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="removing-the-guests_included-column"><a class="header" href="#removing-the-guests_included-column">Removing the 'guests_included' column</a></h1>
<p>The 'guests_included' column will be disregarded from the model's analysis due to its significant skew towards a single value, specifically 1, indicating a maximum limit of one guest per residency.</p>
<p><img src="src%5C5-Treatment-of-outliers%5C5-1-Dropping-uncessary-columns/image/README/1712438881112.png" alt="1712438881112" /></p>
<p>Removing this column from the analysis as a whole is essential as excluding only the outliers could significantly influence the final result.</p>
<p>Moreover, this concentration likely stems from data entry errors, as typical housing accommodations should accommodate more than one person.</p>
<pre><code class="language-python">main_dataframe = main_dataframe.drop('guests_included', axis = 1)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="removing-the-number_of_reviews-column"><a class="header" href="#removing-the-number_of_reviews-column">Removing the 'number_of_reviews' column</a></h1>
<p>The 'number_of_reviews' column will be disregarded from the model's analysis as its goal is to analyze the price for normal users and landlords, which in most cases won't have reviews or a large number of them. Also simpler models are faster and less prone to overfitting.</p>
<pre><code class="language-python">main_dataframe = main_dataframe.drop('number_of_reviews', axis = 1)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="removing-the-maximum_nights-column"><a class="header" href="#removing-the-maximum_nights-column">Removing the 'maximum_nights' column</a></h1>
<p>The 'maximum_nights' feature will be excluded from the model's analysis due to its negligible contribution to price variance. Furthermore, the data exhibits apparent randomness with disparate values across entries, suggesting rapid insertion by users, thus warranting its omission from the analytical framework.</p>
<pre><code class="language-python">print(main_dataframe['maximum_nights'].value_counts())
</code></pre>
<p><img src="src%5C5-Treatment-of-outliers%5C5-1-Dropping-uncessary-columns/image/maximum_nights/1712965913031.png" alt="1712965913031" /></p>
<pre><code class="language-python">main_dataframe = main_dataframe.drop('maximum_nights', axis = 1)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="outlier-function"><a class="header" href="#outlier-function">Outlier Function</a></h1>
<p>Now we will create the 'exclude_outliers' function, that will recieve the main dataframe and the collumn that it will remove the outliers from. Usally, outliers above the upper limit represent luxury properties, which are not considered for the prediction model who's objective is to analyze common apparments, and thus should be removed.</p>
<pre><code class="language-python">def exclude_outliers(main_df, collumn) :
    amount_lines = main_df.shape[0]
    lower_limit, upper_limit = calculate_limits(main_df[collumn])
    main_df = main_df.loc[(main_df[collumn] &gt;= lower_limit) &amp; (main_df[collumn] &lt;= upper_limit), :]
    return main_df, amount_lines - main_df.shape[0]
</code></pre>
<p>In this code the use of the .loc function  with the condition <code>[(main_df[column] &gt;= lower_limit) &amp; (main_df[column] &lt;= upper_limit)]</code>  inside constructs a boolean array that selects rows where the values in the specified column (<code>column</code>) are greater than or equal to <code>lower_limit</code> AND less than or equal to <code>upper_limit</code>.</p>
<p>So, essentially, this line filters the DataFrame <code>main_df</code> to include only the rows where the values in the specified column (<code>column</code>) fall within the range defined by <code>lower_limit</code> and <code>upper_limit</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="removing-outliers-from-the-price-and-extra_people-columns"><a class="header" href="#removing-outliers-from-the-price-and-extra_people-columns">Removing outliers from the 'price' and 'extra_people' columns</a></h1>
<p>The first outliers to be removed will be from the 'price' and 'extra_people' collumns as they have the highest relevance in calculating the final price and they are continues numerical values (can be measured).</p>
<pre><code class="language-python">for collumn in ['price', 'extra_people'] :
    main_dataframe, amount_removed_lines = exclude_outliers(main_dataframe, collumn)

    print(f"{amount_removed_lines} lines were removed from {collumn}")
    ###histogram(main_dataframe[collumn])
    ###box_plot(main_dataframe[collumn])
</code></pre>
<p><img src="src%5C7-Removing-outliers-price-extra-people/image/README/1712967427666.png" alt="1712967427666" /></p>
<p>Nearly 10% of the lines were removed from the 'price' column</p>
<p>Obs: when the price collumn is a integer, the quantity of apparments increases because landlords usally put their price as a whole value.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="removing-outliers-of-discrete-numerical-columns"><a class="header" href="#removing-outliers-of-discrete-numerical-columns">Removing outliers of discrete numerical columns</a></h1>
<p>Now we will undergo the same procedure removing the outliers of the discrete numerical columns:</p>
<pre><code class="language-python">for collumn in ['host_listings_count','accommodates', 'bathrooms', 'beds','bedrooms', 'minimum_nights'] :
    main_dataframe, amount_removed_lines = exclude_outliers(main_dataframe, collumn)
    print(f"{amount_removed_lines} lines were removed from {collumn}")
    ###box_plot(main_dataframe[collumn])
    ###bar_graph(main_dataframe[collumn])
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="treating-non-numerical-values"><a class="header" href="#treating-non-numerical-values">Treating non numerical values</a></h1>
<p>We will analyze the columns without true or false and list values:</p>
<ul>
<li>'property_type'</li>
<li>'bed_type'</li>
<li>'room_type'</li>
<li>'cancellation_policy'</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="group_categories-function"><a class="header" href="#group_categories-function">Group_categories function</a></h1>
<p>First, we will create a function to group specific categories of a given collumn in case said collumn contains a significant amount of categories with small values (value determined by the 'amount' parameter)</p>
<pre><code class="language-python">def group_categories(collumn, grouped_category_name, amount) :
    series_category = main_dataframe[collumn].value_counts()

    for category_type in series_category.index :
        if series_category[category_type] &lt; amount :
            main_dataframe.loc[main_dataframe[collumn] == category_type, collumn] = grouped_category_name
</code></pre>
<p>The main filter occurs in the for loop that iterates throught each different category inside a column and if the amount of values of said categories is smaller than the specified amount passed on the parameters, they will all be grouped into a new category, 'grouped_category_name' whose name is also passed as a parameter of the function.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="group-property_type"><a class="header" href="#group-property_type">Group 'property_type'</a></h1>
<p>Starting with the 'property_type' collumn, we will group all the entires will less than 2000 entries into the 'Other' category since they had a significant amount of categories and their low numbers would make the model more complex and less efficient.</p>
<p>Before grouping</p>
<pre><code class="language-python">bar_graph_string('property_type')
</code></pre>
<p><img src="src%5C10-Group_categories-function%5C10-1-group-property_type/image/README/1713012146037.png" alt="1713012146037" /></p>
<p>After grouping all categories with less than 2000 entries into a single 'Other' category:</p>
<pre><code class="language-python">group_categories('property_type', 'Other', 2000)
bar_graph_string('property_type')
</code></pre>
<p><img src="src%5C10-Group_categories-function%5C10-1-group-property_type/image/README/1713012251643.png" alt="1713012251643" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="group-bed_type"><a class="header" href="#group-bed_type">Group 'bed_type'</a></h1>
<p>Next, the 'bed_type' collumn only has 5 categories, however only the 'real_bed' category has significant values, while the others have small, broken values. Therefore, we will group all these entries into a 'Other beds' category. (The amount 10,000 was chosen simply to choose the remaining categories)</p>
<p>Before grouping:</p>
<pre><code class="language-python">bar_graph_string('bed_type')
</code></pre>
<p><img src="src%5C10-Group_categories-function%5C10-2-group-bed_type/image/README/1713005487149.png" alt="1713005487149" /></p>
<p>After grouping all categories with less than 10,000 values:</p>
<pre><code class="language-python">group_categories('bed_type', 'Other beds', 10000)
bar_graph_string('bed_type')
</code></pre>
<p><img src="src%5C10-Group_categories-function%5C10-2-group-bed_type/image/README/1713005653516.png" alt="1713005653516" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="group-cancellation_policty"><a class="header" href="#group-cancellation_policty">Group 'cancellation_policty'</a></h1>
<p>Following up, the 'cancellation_policy' collumn has 3 categories (strict, super_strict_60, super_strict_30) that contain small values in comparison to the rest. Therefore we will group all these entries into the 'strict' category.</p>
<p>Before:</p>
<p><img src="src%5C10-Group_categories-function%5C10-3-group-cancellation-policy/image/README/1713013660725.png" alt="1713013660725" /></p>
<p>After grouping said categories into the 'strict' category</p>
<p><img src="src%5C10-Group_categories-function%5C10-3-group-cancellation-policy/image/README/1713013749214.png" alt="1713013749214" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="group-room_type"><a class="header" href="#group-room_type">Group room_type</a></h1>
<p>Finally, the 'room_type' collumn only has 4 categories whithout any major value discrepancies in the data (only two categories have significant smaller values), so no change/grouping of categories will be needed.</p>
<pre><code class="language-python">bar_graph_string('room_type')
</code></pre>
<p><img src="src%5C10-Group_categories-function%5C10-4-group-room_type/image/README/1713013912781.png" alt="1713013912781" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="treating-amenities-collumn"><a class="header" href="#treating-amenities-collumn">Treating 'amenities' collumn</a></h1>
<p>Now we will treat the 'amenities' collumn. Since analyzing each amenity would require excessive complexity and computing power, we will instead analyze the length of each amenity array (the higher the length the higher the final price will be)</p>
<p>The code below creates a new collumn on the main dataframe consisting of the length of each amenity array and removes the original  collumn containing the lists.</p>
<pre><code class="language-python">main_dataframe["Amount amenities"] = main_dataframe['amenities'].str.split(',').apply(len)
main_dataframe = main_dataframe.drop('amenities', axis = 1)
</code></pre>
<p>Now we will remove the outliers:</p>
<pre><code class="language-python">#* removing outliers
main_dataframe, amount_removed_lines = exclude_outliers(main_dataframe, 'Amount amenities')
print(f"{amount_removed_lines} lines were removed from Amount amenities")
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reducing-visualized-data"><a class="header" href="#reducing-visualized-data">Reducing visualized data</a></h1>
<p>To facilitate rigorous comparisons and statistical analyses leveraging longitude and latitude data sourced from the dataframe, we will employ plotly.express to construct a Mapbox visualization. This visualization will accurately depict each property at its respective geographical coordinates, alongside with its corresponding price point.</p>
<p>However, in order to avoid crashes and slowdowns, only the first 70,000 samples will be visualized.</p>
<pre><code class="language-python">data = main_dataframe.sample(71000)
center = {'lat':data.latitude.mean(), 'lon':data.longitude.mean()}
map_graph = px.density_mapbox(data, lat='latitude', lon='longitude',z='price', radius=2.5, center=center, zoom=10, mapbox_style='open-street-map')
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loading-the-map-into-the-browser"><a class="header" href="#loading-the-map-into-the-browser">Loading the map into the browser</a></h1>
<p>In order to load all of the data into the browser, we will save the map as an html file, then open it in the browser.</p>
<pre><code class="language-python">with open('map.html', 'w', encoding = 'utf-8') as f :
    f.write(map_graph.to_html())

webbrowser.open(os.path.realpath('map.html'))
</code></pre>
<p><img src="src%5C13-loading-map-to-browser/image/README/1713018401093.png" alt="1713018401093" /></p>
<p><img src="src%5C13-loading-map-to-browser/image/README/1713018454334.png" alt="1713018454334" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encoding-explanation"><a class="header" href="#encoding-explanation">Encoding explanation</a></h1>
<p>We need to adjust some non-numerical columns to facilitate the machine learning model's analysis as it can only analyze numbers.</p>
<p>There are two types of data inside these columns: Categories and True or False</p>
<p>Booleans will become 0 (false) and 1 (true)</p>
<p>Categories will be encoded using dummy encoding (creates columns for each category and applies binary values based if they are in the category or not). This process can be visualized in the image below:</p>
<p><img src="src%5C14-Encoding-explanation/image/README/1713020258843.png" alt="1713020258843" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encoding-booleans"><a class="header" href="#encoding-booleans">Encoding booleans</a></h1>
<p>First, we will create a copy of the dataframe to not accidentaly alter the original:</p>
<pre><code class="language-python">main_dataframe_coded = main_dataframe.copy()
</code></pre>
<p>The collumns 'host_is_superhost', 'instant_bookable', and 'is_business_travel_ready' already contain only 'f' and 't' values, so we will utilize the .map function to substitute those values with zeros and ones respectively.</p>
<pre><code class="language-python">#* Encoding booleans
for collumn in ['host_is_superhost', 'instant_bookable', 'is_business_travel_ready'] :
    main_dataframe_coded[collumn] = main_dataframe[collumn].map({'f':0, 't':1})
</code></pre>
<p>Result before and after:</p>
<p><img src="src%5C15-Encoding-booleans/image/README/1713020595082.png" alt="1713020595082" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encoding-text-columns-dummy-encoding"><a class="header" href="#encoding-text-columns-dummy-encoding">Encoding text columns (dummy encoding)</a></h1>
<p>After  having encoded the boolean columns, all that is left is to convert the text column via dummy encoding.</p>
<pre><code class="language-python">#* Encoding text columns with dummy encoding
main_dataframe_coded = pd.get_dummies(data = main_dataframe_coded, columns = ['property_type', 'room_type', 'bed_type', 'cancellation_policy'])
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="7-steps-to-build-a-prediction-model"><a class="header" href="#7-steps-to-build-a-prediction-model">7 steps to build a prediction model</a></h1>
<p>Firstly, it is necessary to comprehend the details of machine learning and its predictive modeling mechanics. As delineated in the introduction, our selection entails the utilization of a supervised learning model. Yet, within this domain resides a myriad of machine learning models, each distinguished by diverse techniques and methodologies for varied task execution, such as regression and classification.</p>
<p>Regression models are used in tasks that require a large numerical analysis that requires the prediction of a continuous numerical value, for example, the price of a household. Each model utilizes a unique regression technique such as the Linear Regression that works by fitting a straight line to a set of data points in such a way that the errors between the observed and predicted values are minimized.</p>
<p>It utilizes the equation *<em>Y = a * x1 + b <em>x 2 + c * x3 + d * x4 ... + z</em></em> where</p>
<ul>
<li><strong>Y</strong> is the predicted price of the house.</li>
<li><strong>x1,x2,x3</strong> are the independent variables (number of rooms, number of bathrooms, size of the lot, etc.).</li>
<li><strong>a,b,c,d..</strong> are the coefficients representing the impact of each independent variable on the predicted price.</li>
<li><strong>z</strong> is the intercept term.</li>
</ul>
<p>During the training process, the model learns the values of the coefficients <strong>(a,b,c,d...)</strong> that minimize the errors between the predicted and actual prices in the training data. These coefficients capture the relationships between the independent variables and the dependent variable <strong>Y</strong>, allowing you to analyze the impact of various factors on the price of the house.</p>
<p>This linear regression model is one of the three models that were analyzed, with the other two being the Random Forest Regressor and Extra Trees (these three especifically were selected due to their wide use and effectivness in most cases), which will be explained in the third section (model selection).</p>
<p>Then there are other models which are revolved around solving classification problems that require a prediction of a discrete category or class label. A example of this would be a model that predicts if a email is SPAM or not. It uses algorithms to categorize input data into one or more discrete classes or categories.</p>
<p>The construction of a prediction model is sperated into seven steps that we will follow:</p>
<ol>
<li>Define if it is classification or regression problem</li>
<li>Choose the metrics to evaluate the model</li>
<li>Choose which models we are going to use</li>
<li>Train the models and test</li>
<li>Compare the results of the models and choose the best one</li>
<li>Analyse the best model</li>
<li>Adjust and improve the best model</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="defining-if-it-is-classification-or-regression-problem"><a class="header" href="#defining-if-it-is-classification-or-regression-problem">Defining if it is classification or regression problem</a></h1>
<p>Taking into consideration the definitions explained in the previous section:</p>
<ul>
<li>Regression models predict a continuous numerical value.</li>
<li>Classification models predict a discrete category or class label.</li>
</ul>
<p>Given that our objective is to predict the final price of a property according to its attributes such as localization, number of amenities, etc, we can conclude that our goal is to solve a <strong>regression problem</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="choosing-the-metrics-to-evaluate-the-model"><a class="header" href="#choosing-the-metrics-to-evaluate-the-model">Choosing the metrics to evaluate the model</a></h1>
<p>We will utilize two statistical metrics to evaluate the accuracy of the model:</p>
<ol>
<li><strong>R</strong> - ranges from 0 to 1, measuring how much of the variation of data the model can explain</li>
<li><strong>Root Mean Square Error (RSME) -</strong> says how much the model is off</li>
</ol>
<p><img src="src%5C19-Choosing-evaluation-metrics/image/README/1713719409513.png" alt="1713719409513" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="choosing-which-models-we-are-going-to-use"><a class="header" href="#choosing-which-models-we-are-going-to-use">Choosing which models we are going to use</a></h1>
<p>As mentioned previously, we will use and analyze three different models:</p>
<ol>
<li>
<p><strong>Linear Regression</strong> - traces a line that minimizes the erros, values closer to the line are better, not efficient with weak/no correlations</p>
</li>
<li>
<p><strong>Random Forest Regressor - Decision trees -</strong> doing questions separating the data into different groups, random forest regressor uses multiple decision trees with random smaller parts of the data and calculates the mean to reach the final result</p>
</li>
<li>
<p><strong>Extra Trees</strong> - Same as random forest regressor, however the random forest chooses the best question (that will filter the most data) while extra trees asks a random question (which could work best depending on the question)</p>
</li>
</ol>
<p><strong>Example:</strong> The Random Forest and Extra trees models utilize decision trees that are widely used in certain cenarios such as in the guessing game Akinator (the picture below displays the machine learning model's decision tree that branches after each question made).</p>
<p><img src="src%5C20-choose-models-to-use/image/README/1713720166982.png" alt="1713720166982" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="training-and-testing-the-models"><a class="header" href="#training-and-testing-the-models">Training and testing the models</a></h1>
<p>In order to train and test our models we will have to randomly separate our data into two sets: training and testing data</p>
<ul>
<li>Training data will be used for the model to know what values to analyze (x variable in the equation will be the properties' features) and what it wants to calculate (y variable in the equation will be the price of the property)</li>
<li>Testing data will be used for the model to analyze new data and check its accuracy after it is fully trained</li>
</ul>
<p><strong>Obs:</strong> To address overfitting, an 80-20 data split strategy is employed, where 80% of the dataset is allocated for training and 20% for testing. This approach ensures rigorous evaluation of the model's generalization ability by assessing its performance on unseen data. By separating datasets for training and testing, the model's tendency to overly adapt to training data nuances is mitigated, thereby enhancing its efficacy in handling new data. The image below demonstrates the three types of fitting present in the training of machine learning models:</p>
<p><img src="src%5C21-Training-and-testing-models/image/README/1713734932873.png" alt="1713734932873" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="comparing-the-results-of-the-models-and-choosing-the-best-one"><a class="header" href="#comparing-the-results-of-the-models-and-choosing-the-best-one">Comparing the results of the models and choosing the best one</a></h1>
<p>Taking into consideration the evaluation metrics mentioned in the second section we will:</p>
<ul>
<li>First choose 1 main metric, such as R, in which the model with the biggest R will be considered the best model</li>
<li>RSME will be used as a tiebreaker when models have very similar R</li>
<li>Time and complexity will also be taken into consideration (less time and less information needed is prefered)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analyzing-the-best-model"><a class="header" href="#analyzing-the-best-model">Analyzing the best model</a></h1>
<p>After having chosen the best model we will analyze the importance of each of the properties' features the model used to analyze and calculate the final price.</p>
<ul>
<li>If its not relevant, we can remove it to observe changes in the result</li>
<li>We will perform changes with the goal of improving the R/RSME, speed, and simplicity of the model</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adjusting-and-improving-the-best-model"><a class="header" href="#adjusting-and-improving-the-best-model">Adjusting and improving the best model</a></h1>
<p>The iterative process of optimizing a machine learning model involves continuous refinements aimed at achieving peak performance in terms of efficiency and accuracy. This approach entails methodically fine-tuning hyperparameters exploring a wide range of parameter combinations.</p>
<p>Further adjustments involve refining the training process to achieve superior performance. The ultimate goal is to reach a point where additional iterations yield diminishing returns, signaling that the model has achieved its optimal balance between complexity and predictive power.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analyze_model-function"><a class="header" href="#analyze_model-function">'analyze_model' function</a></h1>
<p>Now we will implement all the steps mentioned previously on our code.</p>
<p>First we will create a function to evaluate each model. The parameter 'prediction' is the prediction made by the model  and 'y_test' is the true value of the price used to compare the results.</p>
<pre><code class="language-python">def analyze_model(prediction, y_test) :
    r2 = r2_score(y_test, prediction)
    rsme = root_mean_squared_error(y_test, prediction)
    print(f"The model has an R of {r2} and an RSME of {rsme}\n")
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="splitting-the-data"><a class="header" href="#splitting-the-data">Splitting the data</a></h1>
<p>Now, we will randomly split the 80% of the data into <strong>training data</strong> and 20% into <strong>testing data</strong></p>
<ul>
<li><strong>y</strong> variables will be the 'price' collumn</li>
<li><strong>x</strong> variables will be the features of each property</li>
</ul>
<pre><code class="language-python">#? setting up the y and x variables

y, x = (main_dataframe_coded['price'], main_dataframe_coded.drop('price', axis=1))

#? Splitting the data into training and testing data

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=20)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="testing-the-three-models"><a class="header" href="#testing-the-three-models">Testing the three models</a></h1>
<p>Now we will <strong>train</strong> and <strong>test</strong> each of the three models:</p>
<ol>
<li>Linear Regression</li>
<li>Random Forest Regressor</li>
<li>Extra Trees</li>
</ol>
<pre><code class="language-python">#? Now we will test the three models mentioned previously

linear_model, random_forest_model, extra_trees_model = (LinearRegression(), RandomForestRegressor(), ExtraTreesRegressor())

models_list = ["Linear Regression", "Random Forest Regressor", "Extra Trees"]

for i, model in enumerate([linear_model, random_forest_model , extra_trees_model]):
    model.fit(x_train, y_train)
    prediction = model.predict(x_test)
    print(models_list[i])
    analyze_model(prediction, y_test)
</code></pre>
<p>The results of each model are shown below:</p>
<p><img src="src%5C27-testing-the-models/image/README/1713737881970.png" alt="1713737881970" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="choosing-the-best-model"><a class="header" href="#choosing-the-best-model">Choosing the best model</a></h1>
<p>Here are the results once more:</p>
<p><img src="src%5C28-choosing-best-model-final/image/README/1713737949064.png" alt="1713737949064" /></p>
<p>Following the evaluation of all model outputs, the Extra Trees model was selected as the most effective among the tested algorithms.</p>
<p>Although the linear regression model exhibited the fastest computation time, it recorded a notably low R value of 32%, indicating a weak fit to the data.</p>
<p>Additionally, the Extra Trees and Random Forest models demonstrated similar R values, suggesting comparable levels of predictive accuracy. However, the Random Forest model incurred a higher Root Mean Square Error (RMSE) and required substantially more computational time compared to the Extra Trees model, further supporting the selection of the latter for optimal performance.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analyzing-the-best-model-1"><a class="header" href="#analyzing-the-best-model-1">Analyzing the best model</a></h1>
<p>In order to analyze how the model works, we need to observe the importance of each feature the model uses when calculating the final price.</p>
<pre><code class="language-python">feature_importance_dict = dict(zip(x_train.columns, extra_trees_model.feature_importances_ ))
sorted_feature_importance_dict = sorted(feature_importance_dict.items(), key=lambda x:x[1])
feature_importance_dict = dict(sorted_feature_importance_dict)

print(feature_importance_dict)
</code></pre>
<p>A dictionary in ascending order with the importance (in percentage) of each feature is printed as a result:</p>
<p><img src="src%5C29-analyzing-best-model-final/image/README/1713738545571.png" alt="1713738545571" /></p>
<p>After observing the influence of each feature, we can observe the relevance of the localization as the longitude and latitude account for 20% of the data used in the calculations for the price. Also, the amount of ammenities bedrooms are another crucial factors in the price definition as the houses become more attractive to customers, therefore increasing its prices.</p>
<p>However, other features such as 'is_buisness_travel_ready' have no relevance when calculating the final price, therefore we will experiment removing them to test if the model becomes more efficient.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adjusting-an-improving-the-best-model"><a class="header" href="#adjusting-an-improving-the-best-model">Adjusting an improving the best model</a></h1>
<p>Due to the large amount of columns present in the model's analysis, we will remove all of the features that have an importance of less than <strong>0.007%</strong>. This will remove a large amount of redundant data in the model's analysis, making it considerably more efficient.</p>
<p>Also, after the removal, there will only be one 'room_type' column (the 'room_type_Entire home/apt'). So, in order to add more flexibility for the user when choosing a room type, we will keep the '<strong>room_type_Private room</strong>' column which had the second highest importance.</p>
<pre><code class="language-python">room_type_private_room = main_dataframe_coded['room_type_Private room']

for column in feature_importance_dict :
    if feature_importance_dict[column] &lt; 0.007 :
        print(f"Removed {column}")
        main_dataframe_coded = main_dataframe_coded.drop(column, axis=1)

main_dataframe_coded['room_type_Private room'] = room_type_private_room

#? Separating the new data
y, x = (main_dataframe_coded['price'], main_dataframe_coded.drop('price', axis=1))
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=20)
</code></pre>
<p>We will now create a new final Extra Trees model with the new data and observe its performance.</p>
<pre><code class="language-python">final_model = ExtraTreesRegressor()

final_model.fit(x_train, y_train)
prediction = final_model.predict(x_test)
analyze_model(prediction, y_test)

#? creating a new dictionary for the updated features
feature_importance_dict_final = dict(zip(x_train.columns, final_model.feature_importances_))
sorted_feature_importance_dict = sorted(feature_importance_dict_final.items(), key=lambda x:x[1])
feature_importance_dict_final = dict(sorted_feature_importance_dict)
print(feature_importance_dict_final)
</code></pre>
<p>The results are shown below:</p>
<p><img src="src%5C30-adjusting-and-imporivng-final/image/README/1713739115214.png" alt="1713739115214" /></p>
<p>After removing the 'is_buisness_ready' collumn the <strong>R</strong> increased slightly while the <strong>RSME</strong> decreased, signifying a improvment in the model's accuracy.</p>
<p><img src="src%5C30-adjusting-and-imporivng-final/image/README/1713739346113.png" alt="1713739346113" /></p>
<p>After removing the <strong>'property_type'</strong> and <strong>'bed_type'</strong> collumns the models accuracy slightly decreased, however its simplicity and efficiency had a <strong>significant improvement</strong>.</p>
<p><img src="src%5C30-adjusting-and-imporivng-final/image/README/1713739371919.png" alt="1713739371919" /></p>
<p><img src="src%5C30-adjusting-and-imporivng-final/image/README/1713739437430.png" alt="1713739437430" /></p>
<p><img src="src%5C30-adjusting-and-imporivng-final/image/README/1713739582396.png" alt="1713739582396" /></p>
<p>Lastly, the model's accuracy barely changed after removing all collumns with less than 0.007% of importance, however its <strong>efficiency</strong> and <strong>simplicity</strong> has <strong>improved significantly</strong> (from taking approximately <strong>5 minutes</strong> to test down to less than <strong>2 minutes</strong>)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="applying-the-final-changes"><a class="header" href="#applying-the-final-changes">Applying the final changes</a></h1>
<p>Finally, after implementing all necessary feature modifications, proceed with training the model to evaluate its performance in terms of <strong>efficiency</strong> and <strong>accuracy</strong>. If you wish to conduct comparative analysis with an alternative model to assess whether it outperforms the Extra Trees algorithm, you may consider using a <strong>Random Forest Regressor</strong> for this purpose, but feel free to choose <strong>whichever</strong> model you prefer!</p>
<p>After all the changes are done, we can move on to the final deployment fase.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deployment-forms"><a class="header" href="#deployment-forms">Deployment forms</a></h1>
<p>There are various ways to deploy a machine learning project such as:</p>
<ul>
<li>Host it on a website utilizing django/flask</li>
<li>Transform it into a app with Tkinter</li>
<li>Convert the project into a .exe file</li>
<li>Deploying it with Streamlit</li>
</ul>
<p>In this project, we're opting to deploy the machine learning model through a Streamlit-based web application, which will be accessible via a standalone executable (.exe) file. This deployment method was selected because the model's size is substantial, leading to hosting constraints on many platforms. Given the file size limitations imposed by various hosting providers, deploying through an executable Streamlit application offers a practical solution for accommodating large-scale machine learning models.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="exporting-model-as-joblib"><a class="header" href="#exporting-model-as-joblib">Exporting model as joblib</a></h1>
<p>First we will export the model itself using the joblib library to save it inside a .joblib file. We will add the "price" collumn as we had previously removed it during the training process. The joblib file requires the x values as well as the y values.</p>
<pre><code class="language-python">x["Price"] = y
x.to_csv(r"deploy\final_data.csv")

joblib.dump(final_model, "final_model.joblib", compress = 3)
</code></pre>
<p><strong>Obs:</strong> We added a compress level of 3 inside the joblib.dump parameters in order to reduce the size of the file from approximadetly <strong>2 GB</strong> down to <strong>400 MB</strong>, however, the model gets <strong>slower</strong> each time the <strong>compress level increases</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="airbnb_deploy-file"><a class="header" href="#airbnb_deploy-file">airbnb_deploy file</a></h1>
<p>Create a folder called deploy and inside it a python file called airbnb_deploy.py.  This file will be responsible for deploying the project.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="importing-streamlit-and-setting-dictionaries"><a class="header" href="#importing-streamlit-and-setting-dictionaries">Importing streamlit and setting dictionaries</a></h1>
<p>Importing all necessary libraries:</p>
<pre><code class="language-python">import pandas as pd
import streamlit as st
import joblib
import sys
from streamlit.web import cli as stcli
import os
</code></pre>
<p>Setting up the directories and making sure all requirements are installed:</p>
<pre><code class="language-python">script_directory = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_directory)

sys.argv = ["pip", "install", "-r", "requirements.txt"]
</code></pre>
<p>Now, we will write the code below in order to automatically run the streamlit website locally everytime the file is executed:</p>
<pre><code class="language-python">try :
    if __name__ == '__main__':
            sys.argv = ["streamlit", "run", "airbnb_deploy.py"]
            sys.exit(stcli.main()) 
except RuntimeError as e:
    if str(e) == "Runtime instance already exists!":
        pass
</code></pre>
<p><strong>Obs:</strong> This code <strong>should always</strong> be kept on the <strong>bottom</strong> of the file as it will instantly open the website with only the features of the code above it!</p>
<p>Now, in order to make the buttons for each feature used in the prediction model's analysis, we will first create three dictionaries for each diferent data type (numerical, boolean and lists).</p>
<p>Then, we will create a button for each dictionary and update the values of each column depending on the input.</p>
<p>Creating the dictionaries:</p>
<pre><code>x_numerical = {'latitude': 0, 'longitude': 0, 'accommodates': 0, 'bathrooms': 0, 'bedrooms': 0, 'beds': 0, 'extra_people': 0, 'minimum_nights': 0, 'year': 0, 'Amount amenities': 0, 'host_listings_count': 0}

x_boolean = {'host_is_superhost': 0, 'instant_bookable': 0}

x_lists = {'property_type': ['Apartment', 'House'], 'room_type': ['Entire home/apt', 'room_type_Private room'], 'cancellation_policy': ['flexible', 'moderate', 'strict_14_with_grace_period']}
</code></pre>
<p>Now we will create another dictionary containing only the lists created by the dummy variables. This is so we can store the values the user enters.</p>
<pre><code class="language-python">list_values = {'property_type_Apartment' : 0, 'property_type_House' : 0, 'room_type_Entire home/apt' : 0, 'room_type_Private room' : 0, 'cancellation_policy_flexible' : 0, 'cancellation_policy_moderate' : 0, 'cancellation_policy_strict_14_with_grace_period' : 0}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-page-and-config"><a class="header" href="#setting-up-page-and-config">Setting up page and config</a></h1>
<p>Setting up the page title and icon:</p>
<pre><code class="language-python">st.set_page_config(page_title="Airbnb Deployment", page_icon=":shark:")
st.title("Airbnb Machine Learning Model Deployment")
</code></pre>
<p>We will add a link to a google drive containing joblib file of the prediction model for download in case the user doesn't have it already installed on their computer. The following link was used: <a href="https://drive.google.com/file/d/1VMhrCh5l2neipciZF15Y1lBDS02lgeN5/view?usp=sharing">https://drive.google.com/file/d/1VMhrCh5l2neipciZF15Y1lBDS02lgeN5/view?usp=sharing</a></p>
<pre><code class="language-python">st.write("If you don't have the file of the model, download it [here](https://drive.google.com/file/d/1VMhrCh5l2neipciZF15Y1lBDS02lgeN5/view?usp=sharing)")
</code></pre>
<p><strong>Obs:</strong> On streamlit, use [text] (link) to add a specific link to a text.</p>
<p>Next, we will add a upload box for the user to upload the joblib file:</p>
<pre><code class="language-python">model_file = st.file_uploader("Upload your model file", accept_multiple_files=False)
</code></pre>
<p>However, the default upload limit in streamlit is <strong>200 MB</strong> and our file is over <strong>400 MB</strong>. In order to fix this, we will go to the .streamlit folder and inside the config.toml file write the following code:</p>
<pre><code>[server]
maxUploadSize = 600
maxMessageSize = 600
</code></pre>
<p><strong>Obs:</strong> If said files are not present on the folder of your main project, create them with the same exact names.</p>
<p>The streamlit page looks like this now:</p>
<p><img src="src%5C36-dummy-variable-dictionaries/image/README/1713747174605.png" alt="1713747174605" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="creating-the-buttons"><a class="header" href="#creating-the-buttons">Creating the buttons</a></h1>
<p>Now we will create a button for each dictionary and updating the values after the user's input.</p>
<p>Some buttons such as the latitude and logitude, will have float values and others will have integer, we will adjust their values as needed.</p>
<pre><code class="language-python">for item in x_numerical :
    if item == 'latitude' or item == 'longitude' :
        value = st.number_input(f'{item}', step=0.000001, value=0.0, format="%.6f")
    elif item == 'extra_people' :
        value = st.number_input(f'{item}', step=0.01, value = 0.0) #? default decimal places for floats are already two, so no format is needed
    else :
        value = st.number_input(f'{item}', step = 1, value = 0)
    x_numerical[item] = value

for item in x_boolean :
    value = st.selectbox(f'{item}', ('Yes', 'No'))
    if value == 'Yes' :
        x_boolean[item] = 1
    else :
        x_boolean[item] = 0
</code></pre>
<p>We will iterate over each element of the list_value dictionary, as it is the one that stores the values the user enters</p>
<pre><code class="language-python">for item in x_lists :
    value = st.selectbox(f'{item}', x_lists[item])
    list_values[f'{item}_{value}'] = 1
</code></pre>
<p>The streamlit website looks contains the buttons now:</p>
<p><img src="src%5C37-attributing-values-inputs/image/README/1713748296468.png" alt="1713748296468" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="creating-the-preview-value-button"><a class="header" href="#creating-the-preview-value-button">Creating the preview value button</a></h1>
<p>Lastly, we will create a button for the user to see the predicted value:</p>
<pre><code class="language-python">preview_button = st.button("View the predicted value")
</code></pre>
<p>Once the user clicks the button, we will join the list_values, x_numerical and x_boolean dictionaries updated by the user's input and will create a new Dataframe out of it for the machine learning model to use.</p>
<pre><code class="language-python">if preview_button :
    list_values.update(x_numerical)
    list_values.update(x_boolean)

    x_value_dataframe = pd.DataFrame(list_values, index = [0])
</code></pre>
<p>As the data has to be in the same order as the data the model trained upon, we will create a list containing the columns in the same order (excluding the first column which is for indexing and the last column ('Price') which was implemented after the training). The column order list will be used to rearrange the columns in the dataframe:</p>
<pre><code class="language-python">    data = pd.read_csv("final_data.csv")
    column_order_list = list(data.columns)[1:-1]

    x_value_dataframe = x_value_dataframe[column_order_list]

    #? Loading the model and making the prediction
    model = joblib.load(model_file)
    prediction = model.predict(x_value_dataframe)

    st.write(f"The predicted value is R$ {prediction[0]:.2f}")
</code></pre>
<p><strong>Obs:</strong> All of the code block above is located whithin the preview_button condition!</p>
<p>Done! Now we can calculate the estimated price of a airbnb property given the values of the features that we choose on the website for the model!</p>
<p><img src="src%5C38-preview-value-button/image/README/1713753819193.png" alt="1713753819193" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="finalizing"><a class="header" href="#finalizing">Finalizing</a></h1>
<p>Lastly, we need to configure our project to run streamlit via a .exe file.</p>
<ul>
<li>First, inside the deploy folder, create a run.py file and copy this code for the .exe to run the aplication:</li>
</ul>
<pre><code class="language-python">import streamlit
import joblib
import scipy.special._cdflib
from sklearn.metrics import r2_score, root_mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.model_selection import train_test_split

import streamlit.web.cli as stcli
import os, sys


def resolve_path(path):
    resolved_path = os.path.abspath(os.path.join(os.getcwd(), path))
    return resolved_path


if __name__ == "__main__":
    sys.argv = [
        "streamlit",
        "run",
        resolve_path("airbnb_deploy.py"),
        "--global.developmentMode=false",
    ]
    sys.exit(stcli.main())
</code></pre>
<ul>
<li><strong>Enter the command prompt and ensure you are always inside the deploy folder directory, if you are not, run the command <code>cd deploy</code></strong></li>
<li>Next, go to the command prompt inside the deploy folder directory and create a requirements.txt file via the command <code>pip freeze &gt; requirements.txt</code> and move the file into the deploy folder.</li>
<li>Now create a folder called 'hooks' inside the deploy folder and inside it create a hook-streamlit.py file with the following code:</li>
</ul>
<pre><code class="language-python">from PyInstaller.utils.hooks import copy_metadata

datas = copy_metadata("streamlit")
</code></pre>
<ul>
<li>Also make a copy of the airbnb_deploy.py file and move it into the hooks folder</li>
<li>On the command prompt run the command inside the deploy folder directory run the command <code>pyinstaller --onefile --additional-hooks-dir=./hooks run.py --clean</code>
<ul>
<li>This will generate <code>build</code> and <code>dist</code> folders and a <code>run.spec</code> file. Edit the <code>run.spec</code> file to ensure paths are set properly as below:</li>
</ul>
</li>
</ul>
<pre><code># -*- mode: python ; coding: utf-8 -*-


from PyInstaller.utils.hooks import collect_data_files
from PyInstaller.utils.hooks import copy_metadata

datas = [(".venv/Lib/site-packages/streamlit/runtime", "./streamlit/runtime")]
datas += collect_data_files("streamlit")
datas += copy_metadata("streamlit")


block_cipher = None


a = Analysis(
    ["run.py"],
    pathex=["."],
    binaries=[],
    datas=datas,
    hiddenimports=[],
    hookspath=[],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)
pyz = PYZ(a.pure)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.datas,
    [],
    name='run',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=True,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)

</code></pre>
<p><strong>Obs:</strong> the .venv in the path passed on the data variable at the begining of the code is the name of the path of the computer, it might change depending on the user.</p>
<ul>
<li>On the cmd prompt, inside the deploy folder directory, run <code>pyinstaller run.spec --clean</code>
<ul>
<li>If faced with bugs related to paths not found, re-check if there is a Lib directory or if the name of your virtual enviroment is written correctly</li>
<li>Also make sure that you are inside the right directory inside the cmd prompt</li>
</ul>
</li>
<li>Copy and paste the .streamlit folder with the config.toml file into the dist folder, located inside the deploy folder. This will keep the modifications we made regarding the maximum limit of upload size.</li>
</ul>
<p>All done! The .exe is now located inside the dist folder and when it is executed the streamlit website appears where we can upload the prediction model and utilize its features. We can now send this project to anyone and they can use it regardless if they have python installed or not. You will just need to send the</p>
<p><strong>Obs:</strong> If this is your first time running a streamlit/pyinstaller aplication, it might ask you for your email before executing it on the cmd prompt, however you can provide it whithout any issue as they don't send spam.</p>
<div style="break-before: page; page-break-before: always;"></div>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
